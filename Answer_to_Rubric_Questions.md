## Question 1: Interview Readiness:
What are 3 advantages of deploying using Model Serving methods Vs. deploying on GitHub Pages or HuggingFace for free?

## Answer 1: 
Model serving is making an AI application accessible with an end point (deploying it online) via an API. 
Advantages of deploying a model via cloud services:
- Scalability. Manageable and relatively easy to scale the application to serve large audience or needs (demands).
- Security. Protect the model from being used without permission. 
- Performance. Handling requirements such as latency better with model serving methods. 

## Question 2: Interview Readiness:
What is ML model deployment?

## Answer 2:
- ML model deployment is making an AI application (i.e trained model) accessible and available to users. Deployment involves setting up an infrastructure so that model can be easily used by end users for inference purposes.

## Question 3: Interview Readiness:
What is Causal Inference and How Does It Work?

## Answer 3:
- Causal inference is making a prediction/inference (determining the effect) based on a cause (cause-and-effect). It involves statistical analysis to determine the causal effect of variable on the output. For example, using an observed event or data point to draw/predict the causal relationship (conclusions). 

## Question 4: Interview Readiness:
What is serverless deployment and how its compared with deployment on server?

## Answer 4:
- In serverless deployment, the developer doesn't need to focus on managing/maintaining the server or the infrastructure to run the model on. Instead, the developer can focus more on the model enhancement, development and deployment. Serverless deployment offers simplicity (although setting up could be complex). The biggest difference is in the no need for managing the underlying infrastructure to run an application. 
- Deployment on server requires managing and maintaining the server, storage, networking and infrastructure. 